{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Built In libraries\n",
    "import json\n",
    "from pathlib import Path # used to get file names\n",
    "\n",
    "# Misc Libraries\n",
    "import ffmpeg # Audio processor\n",
    "import numpy as np\n",
    "\n",
    "# Hugging Face and Pytorch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hugging Face Parameters\n",
    "\n",
    "# Device\n",
    "# - Hugging face is device-agnostic & can run on CUDA GPUs, MPS GPUs, or the CPU.\n",
    "device = torch.device(\"cuda\") # Changes the device for Pytorch. Change to suit.\n",
    "device_map = \"cuda:0\" # Device_map is a newer method to set the device for Hugging Face.\n",
    "\n",
    "\n",
    "# Quantization Makes the models less complicated\n",
    "# - Allows for faster running speed & smaller memory Size\n",
    "quantization_config = QuantoConfig(weights=\"int8\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Of data\n",
    "This notebook loads the data from the \"data/video\" directory. Below you set the names of the video files you want transcribed by the program. The notebook also checks if the video has already been transcribed and will skip if that is the case. After the videos are transcribed, the transcripts will be saved to the transcripts.json file."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# list of files to be converted\n",
    "video_filenames = [\n",
    "    \"WSJ_VisionProReview.mp4\",\n",
    "    \"MarquesBrownlee.mkv\",\n",
    "    \"BrianTong-Review.mkv\",\n",
    "    \"iJustine-Unboxing&Review.mkv\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The transcripts file holds all the transcripts that the ASR model has transcribed\n",
    "data_file = \"data/transcripts.json\"\n",
    "\n",
    "with open(data_file, 'r') as f:\n",
    "    t_data = json.load(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare existing files to files available and return a list of videos that need to be transcribed.\n",
    "\n",
    "# Get file names currently in dictionary\n",
    "existing_files = [t for t in t_data]\n",
    "\n",
    "# Files to be transcribed\n",
    "to_transcribe = []\n",
    "\n",
    "# compare the two\n",
    "for f in video_filenames:\n",
    "\n",
    "    # Removes extension or directories\n",
    "    name = Path(f).stem\n",
    "\n",
    "    # Compare the name to existing files\n",
    "    if name not in existing_files:\n",
    "        to_transcribe.append(f) # add to be transcribed list\n",
    "\n",
    "to_transcribe"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe videos to audio\n",
    "In this step the files that are in need of transcribing are converted from their video format into an appropriate audio format. It is possible to directly convert a video to a transcript. However, the ASR models can be picky on what the video format is. Therefore, I recommend adding this step to convert all videos to the same audio format. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Does the actual converting\n",
    "def fileConverter(filename=str, in_dir=\"data/video/\", out_dir=\"data/audio/\"):\n",
    "    \n",
    "    in_filename = in_dir + filename\n",
    "\n",
    "    # Need to change to .wav\n",
    "    just_file = Path(filename).stem\n",
    "    output_filename = out_dir + just_file + \".wav\"\n",
    "\n",
    "    # Fix file formats using ffmpeg\n",
    "    # NOTE This basically creates a loop to go down\n",
    "    (\n",
    "        ffmpeg # Launch fmmpeg\n",
    "        .input(in_filename) # Input the file\n",
    "        .output(output_filename) # Output the file\n",
    "        .overwrite_output() # overwrite the file if it already exist\n",
    "        .run() # Runs the commands above\n",
    "    )\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "output_files = [fileConverter(i) for i in to_transcribe]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe audio to text\n",
    "A automatic speech recognition (ASR) model is used to transcribe audio to text. In this case I used the OpenAI whisper model, however there are many other models available. The Hugging Face pipelines api makes it as easy as changing the model name to switch to a different model if desired. Becuase of the pipeline there should be no change in how the program is used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the ASR pipeline from hugging face\n",
    "asr_pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\", device=device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# run through each output file\n",
    "for f in output_files:\n",
    "    \n",
    "    # Run the transcription\n",
    "    transcript = asr_pipe(f)\n",
    "\n",
    "    # Add the data to the transcription file\n",
    "    title = Path(f).stem # Get the file name\n",
    "    t_data[title] = transcript # save the transcript to that file name"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save transcripts to the transcripts file\n",
    "with open(\"data/transcripts.json\", 'w') as f:\n",
    "    json.dump(t_data, f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "For sentiment analysis I used the bart model. The MNLI version of the model lets you ask the model how related the text is to the keywords you input. In this case I asked for how positive and negative the text was, and if the reviewer found the Apple Vision Pro practical or not. After the model makes the predictions for the text. The probabilities are saved along with the transcripts file. The last notebook cell in this section provides the average sentiment for all of the transcripts in the transcripts file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "classify_pipe = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The function below allows you to input text & labels/keywords you are looking for in the text.\n",
    "def sentiment_analysis(data, labels=['positive', 'negative'], output_key=\"sentiment\"):\n",
    "    for t in data:\n",
    "        \n",
    "        # Get the text\n",
    "        text = data[t]['text']\n",
    "        \n",
    "        # get the scores\n",
    "        scores = classify_pipe(text, labels)['scores']\n",
    "\n",
    "        # Round the scores\n",
    "        scores = [round(s, 3) for s in scores]\n",
    "\n",
    "        # Save the scores\n",
    "        data[t][output_key] = dict(zip(labels, scores))\n",
    "\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Find if reviewers look at the Apple Vision Pro positively.\n",
    "t_data = sentiment_analysis(t_data)\n",
    "# Find if reviewers find the Apple Vision Pro as practical\n",
    "t_data = sentiment_analysis(t_data, labels=['practical', 'unpractical'], output_key=\"practicality\")\n",
    "\n",
    "# Save data to file\n",
    "# Save transcripts as json\n",
    "with open(\"data/transcripts.json\", 'w') as f:\n",
    "    json.dump(t_data, f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Average the sentiment scores\n",
    "practicality_scores = []\n",
    "sentiment_scores = []\n",
    "\n",
    "for t in t_data:\n",
    "    practicality_scores.append(t_data[t]['practicality']['practical'])\n",
    "    sentiment_scores.append(t_data[t]['sentiment']['positive'])\n",
    "\n",
    "print(\"Sentiment\", round(np.mean(sentiment_scores), 4))\n",
    "print(\"Practicality\", round(np.mean(practicality_scores), 4))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "To summarize the videos I used a large language model (LLM) to summarize the top 3 good and bad things the reviewer said about the Apple Vision Pro. There are many LLM models available to download from Hugging Face, and for this notebook I chose to use the Mistral model. This process does not use the pipelines API but the LLM specific Hugging Face Process. Just like pipelines it is as simple as changing the model name to use a different model. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Read the file\n",
    "data_file = \"data/transcripts.json\"\n",
    "with open(data_file, 'r') as f:\n",
    "    t_data = json.load(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the LLM model\n",
    "llm_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model, device_map=\"cuda:0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_model, device_map=\"cuda:0\")#, quantization_config=quantization_config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run the model on the data\n",
    "# WARNING - This will take a long time to run\n",
    "for transcript in t_data:\n",
    "    current_transcript = t_data[t_data]['text']\n",
    "    \n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I need you to summarize the top 4 good and bad things the reviewer said about the Apple Vision Pro\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I will summarize the top 3 good and bad things the reviewer said about the Apple Vision Pro.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\": current_transcript\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Text must be tokenized for the model to interpret\n",
    "    tokenized_chat = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\").to('cuda')\n",
    "    input_length = tokenized_chat.shape[1]\n",
    "    \n",
    "    # This generates the response from the model\n",
    "    generated_ids = model.generate(tokenized_chat, max_new_tokens=128, device_map=device_map)\n",
    "\n",
    "    # Save the response to the transcripts dictionary\n",
    "    t_data[t_data]['LLM_response'] = tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save transcripts to the transcripts file\n",
    "with open(\"data/transcripts.json\", 'w') as f:\n",
    "    json.dump(t_data, f)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
